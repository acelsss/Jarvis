#!/usr/bin/env python3
"""æµ‹è¯•LLMè·¯ç”±å’Œè®¡åˆ’ç”Ÿæˆï¼Œå¹¶è®°å½•æ‰€æœ‰LLMäº¤äº’ã€‚"""
import os
import sys
import json
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# åŠ è½½.envæ–‡ä»¶
try:
    from dotenv import load_dotenv
    env_path = Path(__file__).parent / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… å·²åŠ è½½ .env æ–‡ä»¶: {env_path}")
    else:
        print(f"âš ï¸  .env æ–‡ä»¶ä¸å­˜åœ¨: {env_path}")
except ImportError:
    print("âš ï¸  python-dotenv æœªå®‰è£…ï¼Œè·³è¿‡.envæ–‡ä»¶åŠ è½½")

# ç¡®ä¿å¯ç”¨LLM
os.environ["LLM_ENABLE_ROUTER"] = "1"
os.environ["LLM_ENABLE_PLANNER"] = "1"

from core.llm.client_base import LLMClient
from core.llm.factory import build_llm_client
from core.contracts.task import Task, TASK_STATUS_NEW
from core.context_engine.build_context import build_context, search_openmemory
from core.router.route import route_llm_first
from core.orchestrator.planner import Planner
from core.capabilities.index_builder import build_capability_index
from tools.registry import ToolRegistry
from tools.local.file_tool import FileTool
from tools.local.shell_tool import ShellTool
from skills.registry import SkillsRegistry


class LoggingLLMClient:
    """å¸¦æ—¥å¿—è®°å½•çš„LLMå®¢æˆ·ç«¯åŒ…è£…å™¨ã€‚"""
    
    def __init__(self, base_client: LLMClient):
        self.base_client = base_client
        self.call_count = 0
        self.calls = []
    
    def complete_json(
        self, purpose: str, system: str, user: str, schema_hint: str
    ) -> Dict:
        """è°ƒç”¨LLMå¹¶è®°å½•è¯·æ±‚å’Œå“åº”ã€‚"""
        self.call_count += 1
        call_id = self.call_count
        
        print("\n" + "=" * 80)
        print(f"LLM è°ƒç”¨ #{call_id} - {purpose.upper()}")
        print("=" * 80)
        
        # è®°å½•è¯·æ±‚
        print(f"\n[è¯·æ±‚] Purpose: {purpose}")
        print(f"[è¯·æ±‚] Schema Hint: {schema_hint[:100]}...")
        print(f"\n[System Prompt]:")
        print("-" * 80)
        print(system)
        print("-" * 80)
        
        print(f"\n[User Prompt]:")
        print("-" * 80)
        # å¦‚æœuserå¤ªé•¿ï¼Œåªæ˜¾ç¤ºå‰2000å­—ç¬¦
        if len(user) > 2000:
            print(user[:2000])
            print(f"\n... (çœç•¥ {len(user) - 2000} å­—ç¬¦) ...")
        else:
            print(user)
        print("-" * 80)
        
        # è°ƒç”¨LLM
        print(f"\n[æ­£åœ¨è°ƒç”¨LLM...]")
        try:
            result = self.base_client.complete_json(
                purpose=purpose,
                system=system,
                user=user,
                schema_hint=schema_hint,
            )
            
            # è®°å½•å“åº”
            print(f"\n[å“åº”] æˆåŠŸ")
            print(f"[å“åº”] ç±»å‹: {type(result).__name__}")
            print(f"\n[å“åº”å†…å®¹]:")
            print("-" * 80)
            response_str = json.dumps(result, ensure_ascii=False, indent=2)
            print(response_str)
            print("-" * 80)
            
            # ä¿å­˜è°ƒç”¨è®°å½•
            self.calls.append({
                "call_id": call_id,
                "purpose": purpose,
                "timestamp": datetime.now().isoformat(),
                "system": system,
                "user": user,
                "schema_hint": schema_hint,
                "response": result,
            })
            
            return result
            
        except Exception as e:
            print(f"\n[å“åº”] é”™è¯¯: {e}")
            print(f"[å“åº”] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            
            # ä¿å­˜é”™è¯¯è®°å½•
            self.calls.append({
                "call_id": call_id,
                "purpose": purpose,
                "timestamp": datetime.now().isoformat(),
                "system": system,
                "user": user,
                "schema_hint": schema_hint,
                "error": str(e),
            })
            
            raise


async def test_skill_creator_with_llm():
    """æµ‹è¯•skill-creatorçš„LLMè·¯ç”±å’Œè®¡åˆ’ç”Ÿæˆã€‚"""
    print("=" * 80)
    print("æµ‹è¯•: Skill-Creator LLM è·¯ç”±å’Œè®¡åˆ’ç”Ÿæˆ")
    print("=" * 80)
    
    # æ£€æŸ¥LLMé…ç½®
    provider = os.getenv("LLM_PROVIDER", "").strip().lower()
    if not provider:
        print("âŒ é”™è¯¯: LLM_PROVIDER æœªè®¾ç½®")
        return False
    
    print(f"\nâœ… LLM Provider: {provider}")
    
    # æ„å»ºåŸºç¡€LLMå®¢æˆ·ç«¯
    base_client = build_llm_client()
    if base_client is None:
        print("âŒ é”™è¯¯: æ— æ³•æ„å»ºLLMå®¢æˆ·ç«¯ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return False
    
    # åˆ›å»ºå¸¦æ—¥å¿—çš„å®¢æˆ·ç«¯
    llm_client = LoggingLLMClient(base_client)
    
    # åˆå§‹åŒ–ç»„ä»¶
    skills_registry = SkillsRegistry(workspace_dir="./skills_workspace")
    skills_registry.scan_workspace()
    
    tool_registry = ToolRegistry()
    file_tool = FileTool(sandbox_root="./sandbox")
    shell_tool = ShellTool()
    tool_registry.register(file_tool)
    tool_registry.register(shell_tool)
    
    # æµ‹è¯•ä»»åŠ¡
    test_description = "æˆ‘æƒ³åˆ›å»ºä¸€ä¸ªæ–°çš„skillï¼Œç”¨äºç”ŸæˆæŠ€æœ¯æ–‡æ¡£"
    print(f"\nğŸ“ æµ‹è¯•ä»»åŠ¡: {test_description}")
    
    # åˆ›å»ºä»»åŠ¡
    from core.orchestrator.task_manager import TaskManager
    task_manager = TaskManager()
    task = task_manager.create_task(test_description)
    task.update_status(TASK_STATUS_NEW)
    
    # æ„å»ºä¸Šä¸‹æ–‡
    print("\n[1/4] æ„å»ºä¸Šä¸‹æ–‡...")
    openmemory_results = await search_openmemory(test_description, top_k=3)
    context = build_context(task, openmemory_results=openmemory_results)
    task.context = context
    print(f"âœ… ä¸Šä¸‹æ–‡å·²æ„å»º (OpenMemoryç»“æœ: {len(openmemory_results)} æ¡)")
    
    # è·¯ç”±ä»»åŠ¡
    print("\n[2/4] LLM è·¯ç”±ä»»åŠ¡...")
    available_tools = tool_registry.list_all()
    available_skills = skills_registry.list_all()
    
    capability_index = build_capability_index(skills_registry, tool_registry)
    print(f"âœ… èƒ½åŠ›ç´¢å¼•å·²æ„å»º (æŠ€èƒ½: {len(capability_index.get('skills', []))}, å·¥å…·: {len(capability_index.get('tools', []))})")
    
    route_decision = route_llm_first(
        task.description,
        context,
        capability_index,
        llm_client,
        audit_logger=None,
    )
    
    print(f"\nâœ… è·¯ç”±å†³ç­–:")
    print(f"   - è·¯ç”±ç±»å‹: {route_decision.get('route_type')}")
    print(f"   - æŠ€èƒ½ID: {route_decision.get('skill_id')}")
    print(f"   - ç½®ä¿¡åº¦: {route_decision.get('confidence')}")
    print(f"   - åŸå› : {route_decision.get('reason')}")
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…åˆ°skill-creator
    matched_skill = None
    skill_fulltext = ""
    
    if route_decision.get("route_type") == "skill":
        skill_id = route_decision.get("skill_id")
        matched_skill = available_skills.get(skill_id)
        if matched_skill:
            print(f"\nâœ… åŒ¹é…åˆ°æŠ€èƒ½: {matched_skill.name} ({skill_id})")
            # æ ¹æ®æ¸è¿›å¼åŠ è½½åŸåˆ™ï¼šåªåŠ è½½ SKILL.mdï¼Œä¸è‡ªåŠ¨åŠ è½½å¼•ç”¨æ–‡ä»¶
            skill_fulltext = skills_registry.load_skill_fulltext(skill_id, include_references=False)
            print(f"âœ… æŠ€èƒ½ä¸»æ–‡ä»¶å·²åŠ è½½ ({len(skill_fulltext)} å­—ç¬¦ï¼Œç¬¦åˆæ¸è¿›å¼åŠ è½½åŸåˆ™)")
        else:
            print(f"\nâš ï¸  è·¯ç”±å†³ç­–æŒ‡å‘æŠ€èƒ½ {skill_id}ï¼Œä½†æœªæ‰¾åˆ°è¯¥æŠ€èƒ½")
    else:
        print(f"\nâš ï¸  è·¯ç”±å†³ç­–ç±»å‹ä¸º {route_decision.get('route_type')}ï¼Œä¸æ˜¯skill")
    
    # ç”Ÿæˆè®¡åˆ’
    if matched_skill:
        print("\n[3/4] LLM ç”Ÿæˆè®¡åˆ’...")
        planner = Planner(sandbox_root="./sandbox")
        
        plan = await planner.create_plan(
            task,
            available_tools,
            [],
            skill_fulltext=skill_fulltext,
            llm_client=llm_client,
            audit_logger=None,
        )
        
        print(f"\nâœ… è®¡åˆ’å·²ç”Ÿæˆ:")
        print(f"   - è®¡åˆ’ID: {plan.plan_id}")
        print(f"   - æ­¥éª¤æ•°: {len(plan.steps)}")
        print(f"   - æ¥æº: {plan.source}")
        
        print(f"\nè®¡åˆ’æ­¥éª¤è¯¦æƒ…:")
        for i, step in enumerate(plan.steps, 1):
            print(f"\n   æ­¥éª¤ {i}:")
            print(f"     - å·¥å…·: {step.tool_id}")
            print(f"     - æè¿°: {step.description}")
            print(f"     - é£é™©: {step.risk_level}")
            if step.params:
                print(f"     - å‚æ•°: {json.dumps(step.params, ensure_ascii=False, indent=8)}")
    else:
        print("\nâš ï¸  æœªåŒ¹é…åˆ°æŠ€èƒ½ï¼Œè·³è¿‡è®¡åˆ’ç”Ÿæˆ")
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print(f"âœ… LLM è°ƒç”¨æ¬¡æ•°: {llm_client.call_count}")
    print(f"âœ… è·¯ç”±å†³ç­–: {route_decision.get('route_type')}")
    if matched_skill:
        print(f"âœ… åŒ¹é…æŠ€èƒ½: {matched_skill.name}")
        print(f"âœ… è®¡åˆ’æ­¥éª¤æ•°: {len(plan.steps) if 'plan' in locals() else 0}")
    else:
        print("âš ï¸  æœªåŒ¹é…åˆ°skill-creator")
    
    # ä¿å­˜è°ƒç”¨è®°å½•
    log_file = Path("./llm_test_log.json")
    with open(log_file, "w", encoding="utf-8") as f:
        json.dump({
            "test_description": test_description,
            "provider": provider,
            "total_calls": llm_client.call_count,
            "calls": llm_client.calls,
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… LLMè°ƒç”¨è®°å½•å·²ä¿å­˜åˆ°: {log_file}")
    
    return matched_skill is not None and matched_skill.skill_id == "skill-creator"


def main():
    """ä¸»å‡½æ•°ã€‚"""
    import asyncio
    
    print("\nå¼€å§‹LLMæµ‹è¯•...")
    print(f"LLM_PROVIDER: {os.getenv('LLM_PROVIDER', 'æœªè®¾ç½®')}")
    print(f"LLM_ENABLE_ROUTER: {os.getenv('LLM_ENABLE_ROUTER', '0')}")
    print(f"LLM_ENABLE_PLANNER: {os.getenv('LLM_ENABLE_PLANNER', '0')}")
    
    try:
        success = asyncio.run(test_skill_creator_with_llm())
        if success:
            print("\nğŸ‰ æµ‹è¯•æˆåŠŸï¼skill-creator è¢«æ­£ç¡®è·¯ç”±å’Œè®¡åˆ’ç”Ÿæˆã€‚")
        else:
            print("\nâš ï¸  æµ‹è¯•å®Œæˆï¼Œä½†skill-creatoræœªè¢«åŒ¹é…ã€‚")
        return 0 if success else 1
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
